{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cb27101d-e058-45e5-9ee6-d41cb97a9f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# ! pip install smart_open\n",
    "# ! pip install emoji\n",
    "# ! pip install openpyxl\n",
    "# ! pip install ipywidgets\n",
    "\n",
    "! conda install xeus-python  -c conda-forge  \n",
    "# conda activate jupyterlab-debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810fafc8-a8e6-4230-a8da-49493061a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import wandb\n",
    "\n",
    "import smart_open\n",
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "import emoji\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5626a97-6fc4-4f1d-827e-335f582da0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d326f8f9de6482c8d5c86f6f8e2e0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da512952-c4c9-4b8b-96ba-881f8714f3d1",
   "metadata": {},
   "source": [
    "# Create training / test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce6d5753-3234-4298-91ad-d66803a38bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\ipykernel_launcher.py:226: DtypeWarning: Columns (31,50) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample count before correction: 7288\n",
      "Negative sample count before correction: 23573\n",
      "Total sample count beforoe correction: 30861\n",
      "Need to correct tweet count: 30861\n",
      "Matched tweet count: 1959\n",
      "Positive sample count after correction: 7991\n",
      "Negative sample count after correction: 22870\n",
      "Total sample count after correction: 30861\n",
      "test_positive_cnt, test_negative_cnt: 1598 9055\n",
      "\n",
      "Positive count: 7991\n",
      "Positive count no duplicates: 4980\n",
      "\n",
      "Negative count: 22870\n",
      "Negative count no duplicates: 15289\n",
      "\n",
      "Training set sample count (before over-sampling positive tweets): 20208\n",
      "Training set positive count: 6393\n",
      "Training set negative count: 13815\n",
      "Train set positive count no duplicates: 4195\n",
      "Train set negative count no duplicates: 10715\n",
      "\n",
      "Training set sample count (after over-sampling positive tweets): 55260\n",
      "Training set positive count: 41445\n",
      "Training set negative count: 13815\n",
      "\n",
      "Test set sample count: 10653\n",
      "Test set positive count: 1598\n",
      "Test set negative count: 9055\n",
      "\n",
      "Test set contains positive tweets in training set: 781\n",
      "Test set contains negative tweets in training set: 3736\n",
      "Test set positive count no duplicates: 1320\n",
      "Test set negative count no duplicates: 7645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24133</th>\n",
       "      <td>時間は待ってくれん。ほっといたら置いてかれるぞ。死ぬ気で着いてけ。抜かしてみろよ。時間をおいてけぼりにしてみろ。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>支部やついったにアップしてくれるのも完全な好意だからお前の都合押し付けんな…無配欲しいなら借金してでも這ってでも現地に行け…死ぬ気で行け。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30666</th>\n",
       "      <td>【社会】「母を殺した。これから電車に飛び込む」と110番通報。男性が電車に轢かれて死亡。大田区</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12231</th>\n",
       "      <td>死にたいー ##俺に死んで欲しい人いいね　死んでほしくない人リツイート</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>もうダメだ原稿終わらない死にたい</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     message  \\\n",
       "24133  時間は待ってくれん。ほっといたら置いてかれるぞ。死ぬ気で着いてけ。抜かしてみろよ。時間をおいてけぼりにしてみろ。                \n",
       "15332  支部やついったにアップしてくれるのも完全な好意だからお前の都合押し付けんな…無配欲しいなら借金してでも這ってでも現地に行け…死ぬ気で行け。   \n",
       "30666  【社会】「母を殺した。これから電車に飛び込む」と110番通報。男性が電車に轢かれて死亡。大田区                         \n",
       "12231  死にたいー ##俺に死んで欲しい人いいね　死んでほしくない人リツイート                                     \n",
       "19999  もうダメだ原稿終わらない死にたい                                                        \n",
       "\n",
       "       Label  \n",
       "24133  0      \n",
       "15332  0      \n",
       "30666  0      \n",
       "12231  1      \n",
       "19999  0      "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_twts(tw):\n",
    "\n",
    "  # remove urls\n",
    "  tw = str(tw)\n",
    "  pattern = 'https{0,1}:\\/\\/t.co\\/[a-zA-Z0-9]+'\n",
    "  tw = re.sub(pattern, \"\", tw)\n",
    "\n",
    "  # remove @\n",
    "  pattern = '@[a-zA-Z0-9_]+ '\n",
    "  tw = re.sub(pattern, \"\", tw)\n",
    "\n",
    "  # remove emojis\n",
    "  tw = emoji.demojize(tw)\n",
    "\n",
    "  return tw\n",
    "\n",
    "def clean_twt_row(row):\n",
    "  return clean_twts(row['message'])\n",
    "\n",
    "# all_sample_file = r'https://raw.githubusercontent.com/gladcolor/tweet_classification/master/samples_8400_demojized.zip'\n",
    "\n",
    "# samples_df = pd.read_csv(all_sample_file)\n",
    "\n",
    "def get_positive_sample():\n",
    "    train_sample_file = r'/content/drive/MyDrive/tweet_classification/d_train.xlsx'\n",
    "    train_samples_df = pd.read_excel(train_sample_file)\n",
    "\n",
    "    test_sample_file = r'/content/drive/MyDrive/tweet_classification/d_test.xlsx'\n",
    "    test_samples_df = pd.read_excel(test_sample_file)\n",
    "    \n",
    "    return train_samples_df, test_samples\n",
    "\n",
    "# test_sample_file = r'/content/drive/MyDrive/tweet_classification/suicide_twts339k.zip'\n",
    "# test_samples_df = pd.read_csv(test_sample_file, engine='c', encoding='utf16')\n",
    "\n",
    "\n",
    "# remove emojis\n",
    "# train_samples_df['message'] = train_samples_df['message'].apply(emoji.demojize)\n",
    "# test_samples_df['message'] = test_samples_df['message'].apply(emoji.demojize)\n",
    "\n",
    "def _correct_label(df, correct_df, text_col=\"message\", label_col=\"Correct_label\"):    \n",
    "    merged_df = df.merge(correct_df, left_on='message', right_on='message', how='left')\n",
    "    print(\"Need to correct tweet count:\", len(df))\n",
    "    mask_idx = merged_df['Correct_label'].isna() \n",
    "    mask_idx = (~mask_idx) & (merged_df['Correct_label'] != merged_df['Label'])\n",
    "    mask_idx = ~ mask_idx\n",
    "    print(\"Matched tweet count:\", (~mask_idx).sum())\n",
    "    \n",
    "    # merged_df['Difference'] = \"\"\n",
    "    # merged_df.loc[~mask_idx, 'Difference'] = (merged_df[~mask_idx]['Label'].astype(str) + \"=>\" +  merged_df[~mask_idx]['Correct_label'].astype(str)).to_list()\n",
    "    merged_df.loc[~mask_idx, 'Label'] = merged_df[~mask_idx]['Correct_label'].to_list()\n",
    "    \n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def correct_label(df):\n",
    "    df1 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\bi_label_in_test_correct_labels.csv')\n",
    "    df2 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\bi_label_in_train_correct_labels.csv')\n",
    "    correct_df = pd.concat([df1, df2])\n",
    "    correct_df = pd.concat([df1, df2]).drop_duplicates()\n",
    "    correct_df['message'] = correct_df.apply(clean_twt_row, axis=1)\n",
    "    correct_df = correct_df.drop_duplicates()\n",
    "    # correct_df = correct_df.rename(columns={'Label': \"Correct_label\"})\n",
    "    \n",
    "    # return correct_df\n",
    "    \n",
    "    df = _correct_label(df, correct_df=correct_df, text_col=\"message\", label_col=\"Correct_label\")\n",
    "    \n",
    "    df = df.drop(columns=['Correct_label'])\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_train_sample_Japanese():    \n",
    "    # positive tweet only\n",
    "    # samples_df1 = pd.read_csv(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/week2_positive1083.csv')\n",
    "    samples_df1 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\week2_positive1083.csv')  # count: 1083\n",
    "    \n",
    "\n",
    "    # has positive and negative\n",
    "    # samples_df2 = pd.read_csv(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/「１」LIZ_2021-12-10T12_49_21.000Z_9492.csv')\n",
    "    samples_df2 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\「１」LIZ_2021-12-10T12_49_21.000Z_9492.csv')  # positive count: 1061\n",
    "\n",
    "    # positive only\n",
    "    # samples_df3 = pd.read_csv(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/positive_5087.csv')\n",
    "    samples_df3 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\positive_5087.csv')  # count: 5087\n",
    "\n",
    "    # all negative\n",
    "    # samples_df4 = pd.read_excel(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/TotalNegative.xlsx')\n",
    "    samples_df4 = pd.read_excel(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\TotalNegative.xlsx')\n",
    "\n",
    "    # positive\n",
    "    # samples_df5 = pd.read_excel(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/SuicideRelate_Round2_BasedOn20000.xlsx')\n",
    "    samples_df5 = pd.read_excel(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\SuicideRelate_Round2_BasedOn20000.xlsx')   # count: 90\n",
    "\n",
    "    # negative from LIZ's second' file\n",
    "    # samples_df6 = pd.read_csv(r'/content/drive/Shareddrives/T5/Japan_Tweets_2021_01_01_2022_01_01/LIZ_negative_2021-12-20T11_42_05.000Z_2021-12-21T02_34_19.000Z_9385.csv')\n",
    "    samples_df6 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\LIZ_negative_2021-12-20T11_42_05.000Z_2021-12-21T02_34_19.000Z_9385.csv')   # count: \n",
    "    samples_df6 = samples_df6[['message']]\n",
    "    \n",
    "    samples_df7 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\SuicideRelate_negative_Round2_BasedOn20000.csv')\n",
    "    # samples_df7 = samples_df7.sample(n=7000 , random_state=42)  # n=7000, means about a half of non-\"自殺\" tweets\n",
    "    \n",
    "    samples_df8 = pd.read_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\zhenye_negative.csv')  # count: 12000 without RT \n",
    "       \n",
    "    # positive in samples_df2\n",
    "    positive_idx = samples_df2['RISK(YES:1,NO:0)'] == 1\n",
    "    samples_df2_positive = samples_df2[positive_idx][['id', 'text']]\n",
    "    samples_df2_positive['message'] = samples_df2_positive['text']\n",
    "    samples_df2_positive = samples_df2_positive.drop(columns=['text', 'id'])\n",
    "\n",
    "    # negative in samples_df2\n",
    "    samples_df2_negative = samples_df2[~positive_idx][['id', 'text']]\n",
    "    samples_df2_negative['message'] = samples_df2_negative['text']\n",
    "    samples_df2_negative = samples_df2_negative.drop(columns=['text', 'id'])    \n",
    "    \n",
    "    samples_df4['message'] = samples_df4['CONTENT']\n",
    "    samples_df4 = samples_df4.drop(columns=['CONTENT', 'TWEETID'])\n",
    "\n",
    "    samples_df5 = samples_df5[['message']]\n",
    "\n",
    "    positive_df = pd.concat([samples_df1, samples_df2_positive, samples_df3, samples_df5]).sample(frac=1, random_state=42)  # samples_df1, samples_df2_positive, ,\n",
    "    negative_df = pd.concat([samples_df2_negative, samples_df6, samples_df8]).dropna().sample(frac=1, random_state=42)   # samples_df4, samples_df2_negative, samples_df6, samples_df7: 19900\n",
    "    positive_df['Label'] = 1\n",
    "    negative_df['Label'] = 0    \n",
    "    positive_df = positive_df[positive_df['message'].str[:2] != \"RT\"]\n",
    "    negative_df = negative_df[negative_df['message'].str[:2] != \"RT\"]\n",
    "    \n",
    "    positive_df['message'] = positive_df.apply(clean_twt_row, axis=1)\n",
    "    negative_df['message'] = negative_df.apply(clean_twt_row, axis=1)\n",
    "    \n",
    "    all_samples_df = pd.concat([positive_df, negative_df]) \n",
    "    \n",
    "    print(\"Positive sample count before correction:\", len(positive_df))\n",
    "    print(\"Negative sample count before correction:\", len(negative_df))\n",
    "    print(\"Total sample count beforoe correction:\", len(all_samples_df))\n",
    "    \n",
    "    # Correct the wrong labels.\n",
    "    # positive_df = correct_label(positive_df)\n",
    "    # negative_df = correct_label(negative_df)\n",
    "    \n",
    "    all_samples_df = correct_label(all_samples_df)\n",
    "    # remove the \"自殺\" tweets\n",
    "    keywords = ['首吊り', '首を吊る', '首つり', '死ぬ気', '自分を傷つける', 'この世を去る', '死ぬに値する', '自分の人生を終わらせたいという願望', '死にたい', '自傷', '私の命を奪う', '死にたい', '死にたいです', '私の遺書', '私の人生を終わらせる', '決して起きない', '生きる価値がない', '飛び降りる', '永遠に眠る', '電車に飛び込む', '私がいないほうがいい', '生きるのに疲れた', '一人で死ぬ', '永遠に眠る', '私の悲しい人生', 'ストレスを感じる', 'ストレスで参っている', '感情の起伏が激しい', '私自身が嫌い', '精神的に弱い', '練炭', '焼身', '服毒', 'もう死にたい', '自殺サイト楽に死ねる方法', '生きることがつらい', '死にたい 助けて', '安楽死方法', '一番楽に死ねる方法', '簡単に死ねる方法', '消えたい', '確実に死ねる方法', '生きる意味が分からない', 'うつ 死にたい']\n",
    "    pattern = '|'.join(keywords)  #\"自殺\"  \"自殺\", \n",
    "    mask_ids = all_samples_df['message'].str.contains(pattern)#.sum()\n",
    "    # all_samples_df = all_samples_df[mask_ids]\n",
    "    # print(\"Total sample count after removing 自殺 tweets:\", len(all_samples_df))\n",
    "    \n",
    "    \n",
    "    positive_df = all_samples_df.query(\"Label == 1\") \n",
    "    negative_df = all_samples_df.query(\"Label == 0\")\n",
    "    \n",
    "    positive_df = positive_df.sample(frac=1, random_state=42) # should be 42\n",
    "    negative_df = negative_df.sample(frac=1, random_state=42)\n",
    "    \n",
    "    print(\"Positive sample count after correction:\", len(positive_df))\n",
    "    print(\"Negative sample count after correction:\", len(negative_df))\n",
    "    print(\"Total sample count after correction:\", len(all_samples_df))\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    test_positive_ratio = 0.15  / 0.85  \n",
    "    positive_test_ratio = 0.2  \n",
    "    test_positive_cnt = int(len(positive_df) * positive_test_ratio)\n",
    "    test_negative_cnt = int(test_positive_cnt / test_positive_ratio)   \n",
    "    \n",
    "    print(\"test_positive_cnt, test_negative_cnt:\", test_positive_cnt, test_negative_cnt)\n",
    "\n",
    "    test_positive_df = positive_df.iloc[:test_positive_cnt]\n",
    "    test_negative_df = negative_df.iloc[:test_negative_cnt]\n",
    "\n",
    "    train_positive_df = positive_df.iloc[test_positive_cnt:]\n",
    "    train_negative_df = negative_df.iloc[test_negative_cnt:]\n",
    "    \n",
    "    \n",
    "    test_df = pd.concat([test_positive_df, test_negative_df]).sample(frac=1, random_state=42)\n",
    "    train_df = pd.concat([train_positive_df, train_negative_df]).sample(frac=1, random_state=42)\n",
    " \n",
    "    # print(\"Negative count int natural_dist_positive_df:\", test_positive_cnt)\n",
    "    # print(\"Negative count int natural_dist_negative_df no duplicates:\", len(natural_dist_positive_df.drop_duplicates(subset='message')))\n",
    "\n",
    "    print(\"\\nPositive count:\", len(positive_df))\n",
    "    print(\"Positive count no duplicates:\", len(positive_df.drop_duplicates(subset='message')))\n",
    "\n",
    "    print(\"\\nNegative count:\", len(negative_df))\n",
    "    print(\"Negative count no duplicates:\", len(negative_df.drop_duplicates(subset='message')))\n",
    "\n",
    "    print(\"\\nTraining set sample count (before over-sampling positive tweets):\", len(train_df))\n",
    "    print(\"Training set positive count:\", len(train_df.query(\"Label == 1\")))\n",
    "    print(\"Training set negative count:\", len(train_df.query(\"Label == 0\")))\n",
    "    \n",
    "    # over sampling     \n",
    "    train_positive_df = train_positive_df.sample(n=int(len(train_negative_df) * 3), replace=True, random_state=42)  # Note when replace=True, *1 will not return all samples!\n",
    "    train_df = pd.concat([train_positive_df, train_negative_df]).sample(frac=1, random_state=42)\n",
    "\n",
    "    print(\"Train set positive count no duplicates:\", len(train_positive_df.drop_duplicates(subset='message')))\n",
    "    print(\"Train set negative count no duplicates:\", len(train_negative_df.drop_duplicates(subset='message')))\n",
    "\n",
    "\n",
    "    # test_positive_df = test_positive_df.sample(n=int(len(test_negative_df) * 1), replace=True, random_state=42)\n",
    "    test_df = pd.concat([test_positive_df, test_negative_df]).sample(frac=1, random_state=42)\n",
    "\n",
    "    print(\"\\nTraining set sample count (after over-sampling positive tweets):\", len(train_df))\n",
    "    print(\"Training set positive count:\", len(train_df.query(\"Label == 1\")))\n",
    "    print(\"Training set negative count:\", len(train_df.query(\"Label == 0\")))\n",
    "\n",
    "    print(\"\\nTest set sample count:\", len(test_df))\n",
    "    print(\"Test set positive count:\", len(test_df.query(\"Label == 1\")))\n",
    "    print(\"Test set negative count:\", len(test_df.query(\"Label == 0\")))\n",
    "    print(\"\")\n",
    "\n",
    "    # print(\"Training set contains positive tweets in test set:\", test_positive_df['message'].isin(train_positive_df['message'].to_list()).sum())\n",
    "    # print(\"Training set contains negative tweets in test set:\", test_negative_df['message'].isin(train_negative_df['message'].to_list()).sum())\n",
    "    \n",
    "    print(\"Test set contains positive tweets in training set:\", test_positive_df['message'].isin(train_positive_df['message'].to_list()).sum())\n",
    "    print(\"Test set contains negative tweets in training set:\", test_negative_df['message'].isin(train_negative_df['message'].to_list()).sum())\n",
    "    \n",
    "    print(\"Test set positive count no duplicates:\", len(test_positive_df.drop_duplicates(subset='message')))\n",
    "    print(\"Test set negative count no duplicates:\", len(test_negative_df.drop_duplicates(subset='message')))\n",
    "\n",
    "    return train_df, test_df\n",
    "# make a test\n",
    "train_samples_df, test_samples_df  = get_train_sample_Japanese()\n",
    "test_samples_df.iloc[-5:]\n",
    "\n",
    "# positive_df, negative_df = get_train_sample_Japanese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ccb27d0-5a15-4ea9-aa2f-99a183a3aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_samples_df[train_samples_df['Difference'] != \"\"].sort_values(\"Difference\")\n",
    "# train_samples_df[train_samples_df['Difference'] != \"\"][\"Difference\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d54d110-58c5-4de0-86f6-2caa1bb4222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_samples_df[test_samples_df['Difference'] != \"\"].sort_values(\"Difference\")\n",
    "# test_samples_df[test_samples_df['Difference'] != \"\"][\"Difference\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13d4af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4195, 1320)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unique = train_samples_df.query(\"Label == 1\")['message'].unique()\n",
    "test_unique  = test_samples_df.query(\"Label == 1\")['message'].unique()\n",
    "len(train_unique), len(test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bec4b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_label_df label_count .sum(): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>positive_sum</th>\n",
       "      <th>label_count</th>\n",
       "      <th>diff</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [message, positive_sum, label_count, diff, length]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples_df = pd.concat([train_samples_df, test_samples_df])\n",
    "gb_df = all_samples_df.groupby(['message'], as_index=False).agg(positive_sum=(\"Label\", 'sum'), \n",
    "                                                 label_count=(\"Label\", 'count'))\n",
    "\n",
    "bi_label_df = gb_df.query(\"label_count > 1 and positive_sum < label_count and positive_sum > 0\")\n",
    "\n",
    "print(\"bi_label_df label_count .sum():\", bi_label_df['label_count'].sum())\n",
    "bi_label_df['diff'] = bi_label_df['label_count'] - bi_label_df['positive_sum']\n",
    "bi_label_df['length'] = bi_label_df['message'].str.len()\n",
    "bi_label_df = bi_label_df.sort_values(['length', 'message'])\n",
    "bi_label_df[['message']].to_csv(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\bi_label_in_train.csv', index=False)\n",
    "bi_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70f4fa99-c1f2-45c2-a1bb-5274eae69360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model_name =  'cl-tohoku/bert-base-japanese-v2'\n",
    "# model_name = r'cl-tohoku/bert-large-japanese'\n",
    "# model_name = 'bert-large-uncased'\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# model = SentenceTransformer(model_name)#.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f238941",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6eafeb-90c4-4983-81e2-904b7d930661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "e:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:613: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76e5de9afdb440eba7dee9763fc340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a971100e9c44416e94d28e7051c891f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4aa6eeeaf445a18b202cba3c80eb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d814d4404cd94f259110b96912aa40a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9105619f9241cbb9fe4c137386f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",  # grid, random, bayes\n",
    "    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        'train_batch_size': {'values': [8, 16, 32, 64]},\n",
    "        \"num_train_epochs\": {\"values\": [2, 3, 5, 8, 10]},\n",
    "        \"learning_rate\": {\"min\": 1e-5, \"max\": 4e-4},\n",
    "    },\n",
    "}\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"Simple Sweep\")\n",
    "# output_dir = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all'\n",
    "\n",
    "def train_a_model(output_dir, train_df=train_samples_df, test_df=test_samples_df):\n",
    "    \n",
    "    # wandb.init()\n",
    "    \n",
    "    train_df['Label'] = train_df['Label'].astype(int)\n",
    "    test_df['Label'] = test_df['Label'].astype(int)\n",
    "\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    \n",
    "    num_labels = 2\n",
    "    model_args = ClassificationArgs()\n",
    "    model_args.num_train_epochs = 5\n",
    "    model_args.reprocess_input_data = True\n",
    "    model_args.overwrite_output_dir = True\n",
    "    # model_args.evaluate_during_training = True\n",
    "    model_args.output_dir = output_dir\n",
    "    model_args.manual_seed = 4\n",
    "    model_args.use_multiprocessing = True\n",
    "    model_args.train_batch_size = 8*8\n",
    "    model_args.eval_batch_size = 16*8\n",
    "    model_args.fp16 = True\n",
    "    # model_args.labels_list = [\"true\", \"false\"]\n",
    "    model_args.learning_rate = 4e-5  # 4e-6: mcc= 0.56 , 4e-4: mcc=0\n",
    "    # model_args.wandb_project = \"Simple Sweep\"\n",
    "    \n",
    "    model = ClassificationModel(\n",
    "    model_type = \"bert\",\n",
    "    # model_name = r'bert-base-uncased',  \n",
    "    model_name = model_name,\n",
    "    # model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_v3_verify',\n",
    "        \n",
    "    # model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_v2',\n",
    "    # model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_F1_86',\n",
    "    num_labels=num_labels, \n",
    "    # args={\"reprocess_input_data\": True,   # 对输入数据进行预处理\n",
    "    #       \"overwrite_output_dir\": True,    # 可覆盖输出文件夹\n",
    "    #       \"save_steps\": -1,\n",
    "    #       \"save_model_every_epoch\": False,\n",
    "    #       \"output_dir\": output_dir,\n",
    "    #       # \"train_batch_size\": 1*8, # default: 8\n",
    "    #       \"eval_batch_size\": 64*8, # default: 8\n",
    "    #       },  \n",
    "        \n",
    "    args = model_args,\n",
    "        \n",
    "    \n",
    "    weight=[1, 1.3],\n",
    "    cuda_device=0,\n",
    "        \n",
    "    # sweep_config=wandb.config,\n",
    "        \n",
    "    )\n",
    "\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "    \n",
    "    model.train_model(train_df,\n",
    "                      # args=model_args,\n",
    "                      # args = {\n",
    "                     # 'fp16':True, \n",
    "                     # \"num_train_epochs\": num_train_epochs,\n",
    "                             # }\n",
    "                      # eval_df=test_df,\n",
    "                     )\n",
    "\n",
    "    # Sync wandb\n",
    "    # wandb.join()\n",
    "    \n",
    "    result, model_outputs, wrong_predictions = model.eval_model(test_df)\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "    print(f\"In the reported positives, about {(100 * result['tp'] / (result['tp'] + result['fp'])):.0f}% are correct, i.e., {result['tp']}/({result['tp']}+{result['fp']}).\")\n",
    "    print(f\"Precision: {(result['tp'] / (result['tp'] + result['fp'])):.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"In the reported positives, contain about {(100 * result['tp'] / (result['fn'] + result['tp'])):.0f}% true positive tweets, i.e., {result['tp']}/({result['fn']}+{result['tp']}).\")\n",
    "    print(f\"Recall: {(result['tp'] / (result['tp'] + result['fn'])):.3f}\")\n",
    "\n",
    "    model.save_model(output_dir=output_dir, results=result)\n",
    "    \n",
    "    \n",
    "\n",
    "    return result, model_outputs, wrong_predictions\n",
    "\n",
    "\n",
    "output_dir = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_5epochs_imbalanced_weights'\n",
    "result, model_outputs, wrong_predictions = train_a_model(output_dir, train_samples_df, test_samples_df)\n",
    "result\n",
    "# wandb.agent(sweep_id, train_a_model)\n",
    "\n",
    "# for idx, result in enumerate(result_list):\n",
    "#     print(f\"\\nModel {idx + 1}\")\n",
    "#     print(f\"In the reported positives, about {(100 * result['tp'] / (result['tp'] + result['fp'])):.0f}% are correct, i.e., {result['tp']}/({result['tp']}+{result['fp']}).\")\n",
    "#     print(f\"Precision: {(result['tp'] / (result['tp'] + result['fp'])):.3f}\")\n",
    "#     print(\"\")\n",
    "\n",
    "#     print(f\"In the reported positives, contain about {(100 * result['tp'] / (result['fn'] + result['tp'])):.0f}% true positive tweets, i.e., {result['tp']}/({result['fn']}+{result['tp']}).\")\n",
    "#     print(f\"Recall: {(result['tp'] / (result['tp'] + result['fn'])):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a12ea-3746-4e44-9394-1443abc0f79a",
   "metadata": {},
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b6acb-619f-49fc-ad9d-2adff014d2dc",
   "metadata": {},
   "source": [
    "## Test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec5d5ead-1320-43ad-bea1-f1e89408aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "e:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1455: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c196a36356471cb3405b70b43b5e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa70a509ed4348019fa599684363f46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.8751645112440022, 'tp': 1479, 'tn': 8823, 'fp': 232, 'fn': 119, 'auroc': 0.9825329701884397, 'auprc': 0.9164437387567458, 'eval_loss': 0.29462504174028126}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the reported positives, about 86% are correct, i.e., 1479/(1479+232).\n",
      "Precision: 0.864\n",
      "\n",
      "In the reported positives, contain about 93% true positive tweets, i.e., 1479/(119+1479).\n",
      "Recall: 0.926\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "model = ClassificationModel(\n",
    "model_type = \"bert\",\n",
    "# model_name = r'bert-base-uncased',  \n",
    "# model_name = model_name,\n",
    "\n",
    "model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_v2',\n",
    "# model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_F1_86',\n",
    "num_labels=num_labels, \n",
    "args={\"reprocess_input_data\": True,   # 对输入数据进行预处理\n",
    "        \"overwrite_output_dir\": True,    # 可覆盖输出文件夹\n",
    "        \"save_steps\": -1,\n",
    "        \"save_model_every_epoch\": False,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"train_batch_size\": 1*8, # default: 8\n",
    "        \"eval_batch_size\": 64*8, # default: 8\n",
    "        },  \n",
    "cuda_device=0\n",
    ")\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(train_samples_df)\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_samples_df)\n",
    "\n",
    "     \n",
    "\n",
    "print(f\"In the reported positives, about {(100 * result['tp'] / (result['tp'] + result['fp'])):.0f}% are correct, i.e., {result['tp']}/({result['tp']}+{result['fp']}).\")\n",
    "print(f\"Precision: {(result['tp'] / (result['tp'] + result['fp'])):.3f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"In the reported positives, contain about {(100 * result['tp'] / (result['fn'] + result['tp'])):.0f}% true positive tweets, i.e., {result['tp']}/({result['fn']}+{result['tp']}).\")\n",
    "print(f\"Recall: {(result['tp'] / (result['tp'] + result['fn'])):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "76a80999-ba10-424d-b5d7-ecadb308e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1655.0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples_df['Label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "977f518e-adc7-4368-9b7c-1d97f83e7e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message                                                                                                                                                        Label\n",
       "死にたい                                                                                                                                                           1        217\n",
       "消えたい                                                                                                                                                           1        27 \n",
       "早く死にたい                                                                                                                                                         1        27 \n",
       "死にたい。                                                                                                                                                          1        12 \n",
       "死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい死にたい                   1        8  \n",
       "                                                                                                                                                                       ..  \n",
       "彼氏の子供を産みたい そしてたぶん無理だから 道連れにして死にたい                                                                                                                              1        1  \n",
       "彼氏もいないので、死ぬ気で推しのために働きます!! あと正月前に痩せる！!                                                                                                                          0        1  \n",
       "待って、今日ガチで体調悪い。言いたくないけど死にたいとか漠然と思いかけてるし逃亡したいとか越してる。いやいやいや、普段は楽しいんだよ？旦那や親、こども、友達と遊んだり飯行ったり。ちょっとした事で病む時もあるけど。でも今日の病みは生理にしては重すぎるぞ。                                 0        1  \n",
       "待ってまじで無理死にたい。ごめんなさい。                                                                                                                                           1        1  \n",
       "ｼﾞﾒｯと:snail::snail:  頑張りすぎないよーに よく聞くけど…わからん!! 自分になるとわからん!!!! なんだかよくわかんないまんま 胃が黒くなってなんだかよくわかんないまんま 消えたいって思うよーになったし 力を抜いたらもぉ、立ち上がれないって そこだけ理解できるし 頑張らないと苦痛でしかない  0        1  \n",
       "Length: 1065, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_predictions)\n",
    "# wrong_predictions\n",
    "# np.argmax(model_outputs)\n",
    "# test_samples_df[model_outputs.argmax(axis=1)]\n",
    "idx = model_outputs.argmax(axis=1).astype(bool)\n",
    "test_samples_df[idx].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a421421d-d9a2-4c46-b3a2-3c571627bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.7267152341368192,\n",
       " 'tp': 1792,\n",
       " 'tn': 1803,\n",
       " 'fp': 279,\n",
       " 'fn': 290,\n",
       " 'auroc': 0.9087294831228009,\n",
       " 'auprc': 0.9187344927151347,\n",
       " 'eval_loss': 1.0693100425932143}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45726fe",
   "metadata": {},
   "source": [
    "# Train an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13793edc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all does not appear to have a file named config.json. Checkout 'https://huggingface.co/T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17972\\3180470912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrong_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_a_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_samples_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_samples_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17972\\3180470912.py\u001b[0m in \u001b[0;36mtrain_a_model\u001b[1;34m(output_dir, train_df, test_df)\u001b[0m\n\u001b[0;32m     22\u001b[0m           },  \n\u001b[0;32m     23\u001b[0m     \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcuda_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             self.config = config_class.from_pretrained(\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m             )\n\u001b[0;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"foo\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         ```\"\"\"\n\u001b[1;32m--> 532\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             logger.warning(\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m                     \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m                 )\n\u001b[0;32m    628\u001b[0m                 \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_commit_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 raise EnvironmentError(\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[1;34mf\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                     \u001b[1;34mf\"'https://huggingface.co/{path_or_repo_id}/{revision}' for available files.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 )\n",
      "\u001b[1;31mOSError\u001b[0m: T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all does not appear to have a file named config.json. Checkout 'https://huggingface.co/T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all/None' for available files."
     ]
    }
   ],
   "source": [
    "def train_a_model(output_dir, train_df, test_df):\n",
    "    train_df['Label'] = train_df['Label'].astype(int)\n",
    "    test_df['Label'] = test_df['Label'].astype(int)\n",
    "\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    \n",
    "    num_labels = 2\n",
    "    num_train_epochs = 10\n",
    "    model = ClassificationModel(\n",
    "    model_type = \"bert\",\n",
    "    # model_name = r'bert-base-uncased',  \n",
    "    model_name = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all',\n",
    "    # model_name = model_name, \n",
    "    num_labels=num_labels, \n",
    "    args={\"reprocess_input_data\": True,   # 对 输入数据进行预处理\n",
    "          \"overwrite_output_dir\": True,    # 可覆盖输出文件夹\n",
    "          \"save_steps\": -1,\n",
    "          \"save_model_every_epoch\": False,\n",
    "          \"output_dir\": output_dir,\n",
    "          \"train_batch_size\": 1*8, # default: 8\n",
    "          \"eval_batch_size\": 64*8, # default: 8\n",
    "          },  \n",
    "    weight=[1, 1.3],\n",
    "    cuda_device=0,\n",
    "    )\n",
    "\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "    \n",
    "    model.train_model(train_df,args = {'fp16':True, \n",
    "        \"num_train_epochs\": num_train_epochs})\n",
    "\n",
    "    \n",
    "    result, model_outputs, wrong_predictions = model.eval_model(test_df)\n",
    "     \n",
    "\n",
    "    print(f\"In the reported positives, about {(100 * result['tp'] / (result['tp'] + result['fp'])):.0f}% are correct, i.e., {result['tp']}/({result['tp']}+{result['fp']}).\")\n",
    "    print(f\"Precision: {(result['tp'] / (result['tp'] + result['fp'])):.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"In the reported positives, contain about {(100 * result['tp'] / (result['fn'] + result['tp'])):.0f}% true positive tweets, i.e., {result['tp']}/({result['fn']}+{result['tp']}).\")\n",
    "    print(f\"Recall: {(result['tp'] / (result['tp'] + result['fn'])):.3f}\")\n",
    "\n",
    "    model.save_model(output_dir=output_dir, results=result)\n",
    "\n",
    "    return result, model_outputs, wrong_predictions\n",
    "\n",
    "\n",
    "output_dir = r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all'\n",
    "result, model_outputs, wrong_predictions = train_a_model(output_dir, train_samples_df, test_samples_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62cccf-a4a7-4752-9ad8-11ed7b7dec25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf44300-dc8a-48e0-a2ef-24c77c5e2f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c21eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_excel(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\zhenye_works\\ZhenYe_FullRecord.xlsx', sheet_name='_2021-12-10T12_11_05.000Z_9526')\n",
    "df2 = pd.read_excel(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\zhenye_works\\ZhenYe_FullRecord.xlsx', sheet_name='_2021-12-12T12_11_05.000Z_9526')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aab671-4a19-4b01-bcf0-18280ecef786",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "219e13fc-55c6-4d0b-8fd5-6a221c92ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a3dbc-7250-4968-ac70-86b2824002f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6515e61-d4a9-40c7-8c41-0f976cbe8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model_name = 'cl-tohoku/bert-base-japanese-v2'\n",
    "# model_name = 'bert-large-uncased'\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# model = SentenceTransformer(model_name)#.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4437d57f-5e2d-4b25-ad0f-34631aa20d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('T:\\\\Shared drives\\\\T5\\\\Japan_Tweets_2020-01-01_2021-01-01\\\\predicted\\\\_2020-12-31T09_50_44.000Z_9649.csv',\n",
       " 'T:\\\\Shared drives\\\\T5\\\\Japan_Tweets_2020-01-01_2021-01-01\\\\predicted\\\\positive_all.csv')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "raw_twts_files = glob.glob(os.path.join(r'T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted', '*.csv'))\n",
    "# raw_twts_files = glob.glob(os.path.join(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Japan_tweets\\2020\\cluster_csvs', '*.csv'))\n",
    "raw_twts_files = sorted(raw_twts_files, reverse=True)\n",
    "# raw_twts_files[-100:]\n",
    "# random.sample(raw_twts_files, 3)\n",
    "# raw_twts_files\n",
    "print(len(raw_twts_files))\n",
    "raw_twts_files.append(raw_twts_files[0])\n",
    "raw_twts_files.pop(0)\n",
    "raw_twts_files[0], raw_twts_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3870bbb-d323-4aa1-bb67-61baba886a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_twts_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2324a78b-c045-4a5c-9d68-70b5da1df83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twt_row(row, keep_emoji=False):\n",
    "    try:\n",
    "        cleaned = clean_twts(row['message'], keep_emoji)\n",
    "        return cleaned\n",
    "    except Exception as e:\n",
    "        print(e, row)  \n",
    "        \n",
    "    \n",
    "\n",
    "# clean_twt_row(row=df, keep_emoji=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "913a26cc-86ac-4566-b9fa-ee1a27fd3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path: ['T:\\\\Shared drives\\\\T5\\\\Japan_Tweets_2021_01_01_2022_01_01\\\\Trained_model_all_20221020v1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-31T09_50_44.000Z_9649.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c860b44385ae49a3a68e9577fee3e045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ce0a86b1504946b6221301301fb579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-31T09_50_44.000Z_9649.csv\n",
      "Processing: 2 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-29T15_18_21.000Z_9589.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac70857f627e461382567f3c7037b2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb465063fbc54908920393600efa2e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-29T15_18_21.000Z_9589.csv\n",
      "Processing: 3 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-27T18_28_28.000Z_9255.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df3c9ee14b343c4b8cd86655c590464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1a7e74e8e84a87bcfc9044dc05a48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-27T18_28_28.000Z_9255.csv\n",
      "Processing: 4 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-25T15_10_05.000Z_9248.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf3d481a534fa087761cb325872917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34561613fb4f45a0982521142825b090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-25T15_10_05.000Z_9248.csv\n",
      "Processing: 5 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-23T12_59_18.000Z_9410.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbf409db2ab4ef6839c776091cf75c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f6fdd40f5b4876943dd5bcc1e3c872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-23T12_59_18.000Z_9410.csv\n",
      "Processing: 6 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-21T11_15_31.000Z_9632.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baa602199624005946c38bf471bcc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5ac9e3c1d043d08aa7ca081367062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-21T11_15_31.000Z_9632.csv\n",
      "Processing: 7 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-20T03_32_59.000Z_9568.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fa871a4fe54efc9e8c40238bd3859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a3f2a8ac894b7cb73bf53d9aa36202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classified tweets in:  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2\\Japan_Tweets_2020-01-01_2021-01-01\\_2020-12-20T03_32_59.000Z_9568.csv\n",
      "Processing: 8 / 160  T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted\\_2020-12-18T14_17_04.000Z_9549.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad0745096344c9812ee1319fafc998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8c5f5c76444dc1b283abc2faddb722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27068\\3067039337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[0mresults_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_tweet_csvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_twts_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;31m# print(\"Found positive tweets:\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;31m# results_df['Trained_model_all'].sum()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27068\\3067039337.py\u001b[0m in \u001b[0;36mpredict_tweet_csvs\u001b[1;34m(raw_twts)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# results_df = predict_by_models(df_cleaned)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mdf_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_message'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_twt_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\simple_trans_env2\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, to_predict, multi_label)\u001b[0m\n\u001b[0;32m   2209\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2210\u001b[0m                             \u001b[0mtmp_eval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_eval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2211\u001b[1;33m                         \u001b[0meval_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtmp_eval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2213\u001b[0m                     \u001b[0mnb_eval_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model_paths = [r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_F1_86']\n",
    "# model_paths = [r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all'] # Trained_model_6_japanese\n",
    "model_paths = [r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Trained_model_all_20221020v1'] \n",
    "\n",
    "print(\"model_path:\", model_paths)\n",
    "\n",
    "# save_path = r'\\\\Desktop-h2oge6l\\h\\Research\\Japan_tweets\\predict_test'\n",
    "save_path = r'T:\\Shared drives\\T5\\Japan_Tweets_2020-01-01_2021-01-01\\predicted2'\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    num_labels = 2\n",
    "    # output_dir = ''\n",
    "    model = ClassificationModel(\n",
    "    model_type = \"bert\",\n",
    "    # model_name = r'bert-base-uncased',  \n",
    "    model_name = model_path,\n",
    "    num_labels=num_labels, \n",
    "    args={\"reprocess_input_data\": True,   # 对输入数据进行预处理\n",
    "            \"overwrite_output_dir\": True,    # 可覆盖输出文件夹\n",
    "            \"save_steps\": -1,\n",
    "            \"save_model_every_epoch\": False,     \n",
    "            \"eval_batch_size\": 64*16,\n",
    "            },  \n",
    "    cuda_device=0\n",
    "    )\n",
    "\n",
    "        # model.tokonizer = tokenizer\n",
    "\n",
    "        # basename = os.path.basename(model_path)\n",
    "    return model\n",
    "\n",
    "# def predict_by_models(df_cleaned, model):\n",
    "#     results_df = df_cleaned.copy()\n",
    "    \n",
    "\n",
    "#         # model.tokonizer = tokenizer\n",
    "\n",
    "#     basename = os.path.basename(model_path)\n",
    "        \n",
    "#     predictions, raw_outputs = model.predict(df_cleaned['cleaned_message'].to_list())\n",
    "#     results_df[basename] = predictions\n",
    "    \n",
    "#     # results_df.to_csv(f\"All_Model_prediction.csv\", index=0)\n",
    "\n",
    "#     return results_df\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def get_final(row):\n",
    "    # print(row)\n",
    "    ary = row[-10:].to_list()\n",
    "    final = stats.mode(ary)[0][0]\n",
    "    return final\n",
    "\n",
    "def clean_twts(tw, keep_emoji=False):\n",
    "    tw = str(tw).strip()\n",
    "    try:\n",
    "        if tw == '':\n",
    "            return tw\n",
    "        # remove urls\n",
    "        pattern = 'https{0,1}:\\/\\/t.co\\/[a-zA-Z0-9]+'\n",
    "        tw = re.sub(pattern, \"\", tw)\n",
    "\n",
    "        # remove @  maybe no need to remove. need dig into the data.\n",
    "        pattern = '@[a-zA-Z0-9_]+ '\n",
    "        tw = re.sub(pattern, \"\", tw)\n",
    "\n",
    "\n",
    "        # remove emoji\n",
    "        if not keep_emoji:\n",
    "            tw = emoji.demojize(tw)\n",
    "\n",
    "        return tw\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e, tw)\n",
    "        \n",
    "def clean_twt_row(row, keep_emoji=False):\n",
    "    cleaned = clean_twts(row['message'], keep_emoji)\n",
    "    return cleaned\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "def predict_tweet_csvs(raw_twts):    \n",
    "\n",
    "    for idx, f in enumerate(raw_twts):\n",
    "        \n",
    "        # raw_df = pd.read_csv(f, engine='c')\n",
    "\n",
    "        basename = os.path.basename(f)\n",
    "        # cleaned_name = os.path.join(cleaned_path, basename)\n",
    "        \n",
    "        print(f\"Processing: {idx + 1} / {len(raw_twts)} \", f)\n",
    "\n",
    "        # df_cleaned = pd.read_csv(cleaned_name, engine='c')\n",
    "\n",
    "        # print(f\"Row count: {len(raw_df)}, cleaned row count: {len(df_cleaned)}\" )\n",
    "\n",
    "        df_cleaned = pd.read_csv(f, engine='python', encoding='utf-8').iloc[:]\n",
    "\n",
    "        df_cleaned['cleaned_message'] = df_cleaned.apply(clean_twt_row, axis=1)\n",
    "\n",
    "\n",
    "        df_cleaned = df_cleaned.fillna(\"\")\n",
    "        df_cleaned['message'] = df_cleaned['message'].str.strip()\n",
    "\n",
    "        \n",
    "\n",
    "        # empty_idxs = (df_cleaned['len'] == 0)\n",
    "        # df_cleaned[empty_idxs]\n",
    "        # df_cleaned.loc[empty_idxs, 'message'] = '--'\n",
    "\n",
    "        # results_df = predict_by_models(raw_df[:], df_cleaned[:])\n",
    "        # results_df = predict_by_models(df_cleaned)\n",
    "        \n",
    "        predictions, raw_outputs = model.predict(df_cleaned['cleaned_message'].to_list())\n",
    "        \n",
    "        df_cleaned['cleaned_message'] = df_cleaned.apply(lambda x: clean_twt_row(x, True), axis=1)\n",
    "        \n",
    "        df_cleaned['len'] = df_cleaned['message'].str.len()\n",
    "        df_cleaned['len'] = df_cleaned['len'].astype(int)\n",
    "        \n",
    "\n",
    "        results_df = df_cleaned\n",
    "\n",
    "        results_df['Label'] = predictions\n",
    "        \n",
    "        \n",
    "            # remove the \"自殺\" tweets\n",
    "        keywords = ['首吊り', '首を吊る', '首つり', '死ぬ気', '自分を傷つける', 'この世を去る', '死ぬに値する', '自分の人生を終わらせたいという願望', '死にたい', '自傷', '私の命を奪う', '死にたい', '死にたいです', '私の遺書', '私の人生を終わらせる', '決して起きない', '生きる価値がない', '飛び降りる', '永遠に眠る', '電車に飛び込む', '私がいないほうがいい', '生きるのに疲れた', '一人で死ぬ', '永遠に眠る', '私の悲しい人生', 'ストレスを感じる', 'ストレスで参っている', '感情の起伏が激しい', '私自身が嫌い', '精神的に弱い', '練炭', '焼身', '服毒', 'もう死にたい', '自殺サイト楽に死ねる方法', '生きることがつらい', '死にたい 助けて', '安楽死方法', '一番楽に死ねる方法', '簡単に死ねる方法', '消えたい', '確実に死ねる方法', '生きる意味が分からない', 'うつ 死にたい']\n",
    "        pattern = '|'.join(keywords)  #\"自殺\"  \"自殺\"\n",
    "        keywords_idx = results_df['message'].str.contains(pattern)\n",
    "        sucide_idx = results_df['message'].str.contains('自殺')\n",
    "        mask_idx = (~keywords_idx) & sucide_idx\n",
    "        \n",
    "        # return results_df\n",
    "        results_df.loc[mask_idx, \"Label\"] = 0\n",
    "\n",
    "        \n",
    "        results_df['len'] = results_df['message'].str.len()\n",
    "        # results_df['final_label'] = results_df.apply(get_final, axis=1)\n",
    "        # results_df['final_label'] = results_df.iloc[:, -1]\n",
    "\n",
    "\n",
    "        # results_df['final_label'] = results_df['final_label'].astype(int)\n",
    "        results_df = results_df.sort_values(['Label', 'len'], ascending=[False, True])\n",
    "        # results_df.to_csv(r'predictions_final_label.csv', index=False)\n",
    "        # results_df.to_csv(f\"All_Model_prediction.csv\", index=0)\n",
    "\n",
    "        # save_path = r'/content/drive/MyDrive/tweet_classification/annual_tweets/classified_model1'\n",
    "        year = f.split('\\\\')[-3]\n",
    "        save_path_year = os.path.join(save_path, year)\n",
    "        os.makedirs(save_path_year, exist_ok=True)\n",
    "\n",
    "        classified_name = os.path.join(save_path_year, basename)\n",
    "        classified_name = classified_name[:-4] + \".csv\"\n",
    "\n",
    "        results_df.to_csv(classified_name, float_format='%.0f', index=False)\n",
    "\n",
    "        print(\"Saved classified tweets in: \", classified_name)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "model_path = model_paths[0]\n",
    "model = load_trained_model(model_path)\n",
    "model.tokenizer = tokenizer\n",
    "results_df = predict_tweet_csvs(raw_twts_files[:])\n",
    "# print(\"Found positive tweets:\")\n",
    "# results_df['Trained_model_all'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "57e788d5-92cb-4c53-8531-51adfbdeb89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predicted'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(os.path.join(save_path, os.path.dirname(r'T:\\Shared drives\\T5\\Japan_Tweets_2021_01_01_2022_01_01\\Japan_tweets\\2001\\csv\\tx.t')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ced3e0b2-6475-4425-b92f-d4deeae4d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['首吊り', '首を吊る', '首つり', '死ぬ気', '自分を傷つける', 'この世を去る', '死ぬに値する', '自分の人生を終わらせたいという願望', '死にたい', '自傷', '私の命を奪う', '死にたい', '死にたいです', '私の遺書', '私の人生を終わらせる', '決して起きない', '生きる価値がない', '飛び降りる', '永遠に眠る', '電車に飛び込む', '私がいないほうがいい', '生きるのに疲れた', '一人で死ぬ', '永遠に眠る', '私の悲しい人生', 'ストレスを感じる', 'ストレスで参っている', '感情の起伏が激しい', '私自身が嫌い', '精神的に弱い', '練炭', '焼身', '服毒', 'もう死にたい', '自殺サイト楽に死ねる方法', '生きることがつらい', '死にたい 助けて', '安楽死方法', '一番楽に死ねる方法', '簡単に死ねる方法', '消えたい', '確実に死ねる方法', '生きる意味が分からない', 'うつ 死にたい']\n",
    "pattern = '|'.join(keywords)  #\"自殺\"  \"自殺\", \n",
    "# mask_ids = results_df['message'].str.contains(pattern)#.sum()\n",
    "# results_df.loc[~mask_ids, 'trained_model_all'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5b90139d-5154-4f1e-b564-d4de79df3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "Name: message, dtype: bool"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([{'message':\"keywords\"}])['message'].str.contains(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70564b37-a69e-47e3-99b4-147702d5c356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "死にたい                                                                                                                                            2586\n",
       "消えたい                                                                                                                                            331 \n",
       "死にたい。                                                                                                                                           229 \n",
       "早く死にたい                                                                                                                                          212 \n",
       "死にたい…                                                                                                                                           116 \n",
       "                                                                                                                                               ...  \n",
       "中途半端にブロン飲んでもメンタル落ちるだけな気がする ちょっとは気持ちいいけど自分の無力感が上回る。すごく死にたい、いなくなりたい、生まれてこなきゃよかった、存在消してほしい 一時でも僕を必要としてくれた人だけ僕のことを覚えていてほしい それ以外の人の記憶から消えたい          1   \n",
       "電車とかは動悸しちゃうから乗れなくて、友達といると気が紛れるけど結局はドタキャンしちゃうし、起きてから寝るまで布団のなかに居る生活がかれこれ4ヶ月くらい続いてるの。過去の上司には死にたいな死ねば？って言われ、そんなやついらないと、辞めさせられたりして。社会復帰とは…？ってなるよね    1   \n",
       "弟が母に僕はうつ病かもしれないとか話したらしいけど私はマジで心が辛くなったとき誰にも話せなくて毎日意味もなく涙が出てご飯もあまり食べられなくて6キロぐらい痩せたけど練習しなきゃって気持ちはあったから死ぬ気で練習したけどな〜 あいつにはとにかく気力が足りない、私はもっと頑張ってた     1   \n",
       "入れ替わりSSR素晴らしすぎて死ぬ気でゲットしなかったの悔やむ:crying_face::crying_face::crying_face:                                                                          1   \n",
       "風邪を引くだけでも激鬱になって死にたいと思うような人間がコロナの苦痛に耐えられる気がしないし、感染して隔離される間の時間ロスは痛いし病原性が下がってようがコロナに感染したくないよね。                                                     1   \n",
       "Name: message, Length: 4265, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples_df.query(\"Label == 1\")['message'].value_counts()\n",
    "# test_samples_df.query(\"Label == 1\")['message'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_trans_env2",
   "language": "python",
   "name": "simple_trans_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
